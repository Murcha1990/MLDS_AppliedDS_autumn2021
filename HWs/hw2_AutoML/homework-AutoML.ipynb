{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weekly-bumper",
   "metadata": {},
   "source": [
    "# Прикладные задачи анализа данных, МОВС ФКН\n",
    "\n",
    "## AutoML и оптимизация гиперпараметров.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 15 баллов.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "### Формат сдачи\n",
    "\n",
    "Задания сдаются через систему anytask. Посылка должна содержать:\n",
    "\n",
    "- Ноутбук `homework-autoML-Username.ipynb`\n",
    "- Модули `distributions.py` и `optimizers.py`, содержащие написанный вами код\n",
    "\n",
    "Username — ваша фамилия на латинице\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит разработать свой собственный AutoML фреймворк, который будет подбирать оптимальные значения гиперпараметров для определенной модели и обучающих данных. Попутно вы познакомитесь с основными концепциями Байесовской оптимизации и узнаете, как Гауссовские процессы применяются на практике. Если вы все сделаете правильно, то на выходе получится фреймворк, не уступающий популярной `optuna`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-fluid",
   "metadata": {},
   "source": [
    "## 1. AutoML фреймворк (7.5 баллов)\n",
    "\n",
    "Наш фреймворк будет работать по аналогии с классом [`OptunaSearchCV`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.OptunaSearchCV.html#optuna.integration.OptunaSearchCV) из пакета `optuna`, который интегрирован с интерфейсом `sklearn`. Общая схема следующая: сначала мы выбираем модель, для которой будем подбирать гиперпараметры, и задаем распределения на них (решаем, будет ли параметр вещественным или целочисленным; определяем границы; выбираем линейную или логарифмическую шкалу). Далее, на каждой итерации мы выбираем новый набор значений гиперпараметров и оцениваем качество модели на них с помощью кросс-валидации. За ограниченное число итераций находим лучший набор гиперпараметров и обучаем итоговую модель на всех данных. Условимся, что мы хотим **максимизировать** значение метрики (в случае, если мы хотим минимизировать метрику, будем брать ее с минусом). Отличие между различными стратегиями поиска заключается в том, как именно выбираются новые значения гиперпараметров. Подробнее об этом ниже.\n",
    "\n",
    "**Задание 1.1 (2 балла).** Реализуйте распределения гиперпараметров в файле `distributions.py`. Каждый класс должен поддерживать метод `sample`, который возвращает выборку из распределения. Кроме того, для численных распределений нам понадобится метод `scale`, который переводит выборку в промежуток $[0, 1]$ (с линейной шкалой) (за подробностями обращайтесь к док-строкам в базовых классах). Фактически, вам нужно реализовать [квантильное преобразование](https://en.wikipedia.org/wiki/Quantile_normalization) к случайной величине, равномерной на $[0, 1]$. При нормализации нужно использовать значения минимума и максимума из параметров распределения, а не посчитанные по переданной выборке. Не забудьте прологарифмировать лог-распределения, чтобы шкала преобразованной выборки стала линейной. Чуть позже станет понятнее, как нам пригодится эта функция.\n",
    "\n",
    "Каждый оптимизатор, который мы реализуем, будет наследоваться от класса `BaseOptimizer` из файла `optimizers.py`, который уже написан за вас. Ознакомьтесь с его методами, подробные описания доступны в док-строках. История наборов параметров и метрик сохраняется в полях `self.params_history` и `self.scores_history`.\n",
    "\n",
    "**Задание 1.2 (1 балл).** Реализуйте простейшую возможную стратегию &mdash; стратегию случайного поиска в классе `RandomSearchOptimizer` из файла `optimizers.py`. На каждой новой итерации алгоритма мы генерируем новый случайный набор гиперпараметров, не производя никакого дополнительного отбора.\n",
    "\n",
    "### Байесовская оптимизация\n",
    "\n",
    "Далее мы перейдем к более продвинутым методам поиска гиперпараметров. Одним из наиболее распространенных подходов к этой задаче является **Байейсовская оптимизация**. Ее суть состоит в следующем: на каждой итерации мы будем аппроксимировать распределение $p(y|x)$, где $y$ &mdash; это значение метрики, а $x$ &mdash; значение набора гиперпараметров. Допустим, что $y^*$ &mdash; это наилучшее (наибольшее) текущее значение метрики. Чтобы выбрать новое значение гиперпараметров, для которого мы будем учить модель, нам понадобится величина под названием **Expected Impovement (EI)**. Она определяется следующим образом:\n",
    "\n",
    "$$\n",
    "\\text{EI}_{y^*}(x) = \\mathbb{E}_{y \\sim p(y|x)} \\Big[\\max(0, y - y^*)\\Big] = \\int \\max(0, y - y^*) p(y|x) dy\n",
    "$$\n",
    "\n",
    "Соответственно, чем выше значение EI, тем большего увеличения $y^*$ мы ожидаем, обучив модель с гиперпараметрами $x$. Таким образом, на каждой итерации оптимизации мы будем семлировать несколько кандидатов $x_i, i=1, \\dots, N$ из исходного распределения гиперпараметров и выбирать из них наилучшего путем максимизации EI: $x_{\\text{new}} = \\arg\\max_{x_i} \\big\\{\\text{EI}_{y^*} (x_i)\\big\\}$. Нам осталось обсудить, как именно аппроксимировать распределение $p(y|x)$.\n",
    "\n",
    "### Гауссовские процессы\n",
    "\n",
    "[Гауссовские процессы](https://en.wikipedia.org/wiki/Gaussian_process) (ГП) &mdash; одна из популярных опций для задания распределения $p(y|x)$. Если не вдаваться в глубокую теорию, то можно воспринимать ГП как случайные функции. ГП задаются двумя параметрами: функцией среднего $\\mu(x)$ и ковариационной функцией $K(x, x')$. Процесс называется гауссовским, поскольку в каждой отдельно взятой точке $x$ мы имеем нормальное распределение: $y(x) \\sim \\mathcal{N}\\big(\\mu(x), K(x, x)\\big)$. А условие $\\text{Cov}\\big(y(x), y(x')\\big) = K(x, x')$ объясняет название ковариационной функции. Как правило, для практических применений достаточно так называемых стационарных ГП, которые отличают константная функция среднего $\\mu(x)=\\text{const}$ (чаще всего даже $\\mu(x)=0$) и ковариационная функция, зависящая только от расстояния между точками: $K(x, x') = K\\big(\\|x-x'\\|\\big)$. Часто под переменной $x$ понимают время (отсюда название \"процесс\"), но мы будет рассматривать в качестве $x$ наше пространство гиперпараметров.\n",
    "\n",
    "На первый взгляд может показаться, что ГП мало чем могут быть полезны: ведь в каждой точке $x$ у нас свое собственное случайное значение $y$. Но оказывается, что если ковариационная функция непрерывна в нуле, то (почти все) реализации ГП (то есть интересующие нас случайные значения $y(x)$) непрерывны. Таким образом, каждая реализация ГП соответствует некоторой непрерывной функции, а значит, ГП можно использовать для регрессии.\n",
    "\n",
    "Если задана обучающая выборка $\\{(x_i, y_i)\\}_{i=1}^{\\ell}$, то мы можем посчитать распределение $p(y|x) = \\mathcal{N}\\big(y\\big|\\mu(x), \\sigma^2(x)\\big)$ для любой точки $x$ (замечание: здесь уже не будет выполняться $\\mu(x)=0$, так как распределение на $y$ апостериорное, то есть зависит от обучающих данных). При этом, для разных ковариационных функций мы будем получать разные распределения. Нам повезло, что все это уже реализовано в классе [`sklearn.gaussian_process.GaussianProcessRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor), и мы можем сконцентрироваться на алгоритмах оптимизации гиперпараметров.\n",
    "\n",
    "Ниже приведен пример регрессии с помощью ГП:\n",
    "![](https://www.researchgate.net/profile/Florent-Leclercq/publication/327613136/figure/fig1/AS:749406701776896@1555683889137/Illustration-of-Gaussian-process-regression-in-one-dimension-for-the-target-test.png)\n",
    "\n",
    "Впрочем, в нашем случае, картинка будет выглядеть скорее так, поскольку $x$ будет многомерным:\n",
    "![](https://raw.githubusercontent.com/jmaronas/TGP.pytorch/master/images/2D_GP.gif)\n",
    "\n",
    "Для нормально распределенной случайной величины EI можно выписать в аналитическом виде. Он выглядит следующим образом, здесь $\\mathcal{N}\\big(y\\big|\\mu(x), \\sigma^2(x)\\big)$ &mdash; плотность нормальной с.в. (pdf, probability density function), а $F\\big(y\\big|\\mu(x), \\sigma^2(x)\\big)$ &mdash; ее функция распределения (cdf, cumulative distribution function).\n",
    "$$\n",
    "\\text{EI}_{y^*}(x) = \\sigma^2(x) \\mathcal{N}\\big(y^*\\big|\\mu(x), \\sigma^2(x)\\big) + (\\mu - y^*)\\Big(1 - F\\big(y^*\\big|\\mu(x), \\sigma^2(x)\\big)\\Big)\n",
    "$$\n",
    "\n",
    "Теперь разберемся, как адаптировать ГП под наши нужды. Предлагается следующий план действий:\n",
    "\n",
    "1. Пускай на очередной итерации алгоритма $j$ у нас есть выборка $\\{(x_i, y_i)\\}_{i=1}^{j-1}$, где $x_i$ &mdash; значение набора гиперпараметров, а $y_i$ &mdash; соответствующее ему значение метрики, посчитанное по кросс-валидации.\n",
    "\n",
    "\n",
    "2. Если размер выборки меньше значения `self.num_dry_runs`, то возвращаем случайное значение гиперпараметров вместо запуска алгоритма. Таким образом, первые `self.num_dry_runs` итераций поиска подчиняются случайной стратегии, чтобы \"набрать статистику\" для работы алгоритма.\n",
    "\n",
    "\n",
    "3. ГП работает только с непрерывными величинами, поэтому давайте переведем значения гиперпараметров в диапазон $[0, 1]$ (тут-то нам и понадобится функция `scale`) и получим отнормированные гиперпараметры $x_i \\to \\tilde{x}_i$. Как вы догадываетесь, такой трюк сработает только с численными гиперпараметрами (как с непрерывными, так и с дискретными). Что делать с категориальными гиперпараметрами, обсудим позже. \n",
    "\n",
    "\n",
    "4. Далее обучаем ГП на выборку $\\{(\\tilde{x}_i, y_i)\\}_{i=1}^{j-1}$ (в качестве ковариационной функции возьмите сумму константного, белого и RBF ядер). \n",
    "\n",
    "\n",
    "5. Семплируем кандидатов в новые значения гиперпараметров $x^{\\text{new}}_1, ..., x^{\\text{new}}_N$ и преобразуем их функцией `scale`. Для каждого набора предсказываем параметры распределения $\\mu(\\tilde{x}), \\sigma^2(\\tilde{x})$, затем вычисляем $\\text{EI}_{y^*}(\\tilde{x})$ по формуле, представленной выше, и выбираем набор гиперпараметров $\\tilde{x}^\\text{new}_{*}$ с наибольшим значением EI.\n",
    "\n",
    "\n",
    "6. Выбираем исходный набор гиперпараметров $x^\\text{new}_{*}$, соответствующий $\\tilde{x}^\\text{new}_{*}$. Вуаля! Мы получили новый набор численных гиперпараметров, на котором будем валидировать модель.\n",
    "\n",
    "\n",
    "Для категориальных гиперпараметров попробуем провернуть что-нибудь похожее. Пусть у нас есть выборка $\\{(f_i, y_i)\\}_{i=1}^{j-1}$, где $f$ &mdash; интересующий нас категориальный гиперпараметр, $f_i \\in \\{1, \\dots, C\\}$ &mdash; конечное множество категорий. Зафиксируем $c \\in \\{1, \\dots, C\\}$ и рассмотрим $p(y|с) = \\mathcal{N}\\big(y\\big|\\mu(с), \\sigma^2(с)\\big)$. Определим параметры распределений следующим образом (как среднее и сглаженную дисперсию по категории):\n",
    "\n",
    "$$\n",
    "\\mu(c) = \\begin{cases}\n",
    "    \\cfrac{\\sum_i [f_i = c] \\cdot y_i}{\\sum_i [f_i = c]},& \\sum_i [f_i = c] > 0 \\\\\n",
    "    y^*,& \\sum_i [f_i = c] = 0\n",
    "\\end{cases}\n",
    "\\quad\\quad\\quad\n",
    "\\sigma^2(c) = \\frac{1 + \\sum_i [f_i = c] (y_i - \\mu(c))^2}{1 + \\sum_i [f_i = c]}\n",
    "$$\n",
    "\n",
    "В итоге, как и в случае с численными гиперпараметрами, мы находим категорию с лучшей EI. Впрочем, обычно такие гиперпараметры имеют не очень много возможных значений (скажем, не больше 10 категорий, а в реальности скорее 2-3: например, критерий при построении дерева), поэтому мы можем себе позволить **перебрать все значения** категориального признака **вместо семплирования** (что и требуется от вас в реализации). Обратите внимание, что категориальные гиперпараметры нужно обрабатывать отдельно (то есть они не имеют ничего общего с ГП, который мы обучаем по численным гиперпараметрам). Конечно, у такого подхода есть значительный минус: категории рассматриваются независимо друг от друга и от численных гиперпараметров, но уж лучше так, чем никак. \n",
    "\n",
    "**Задание 1.5 (4.5 балла)**. Реализуйте недостающие методы в классе `GPOptimizer` в файле `optimizers.py`. Подсказка: воспользуйтесь функциями из модуля `scipy.stats.norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributions as D\n",
    "import optimizers as O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-expense",
   "metadata": {},
   "source": [
    "## 2. Эксперименты (7.5 баллов)\n",
    "\n",
    "**Задание 2.1 (1.5 балла)**. Сгенерируйте случайную выборку наборов гиперпараметров и выборку значений метрик. Замерьте время работы метода `select_params` для `GPOptimizer`. Постройте график зависимости времени работы от размера выборки, сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (⊃｡•́‿•̀｡)⊃"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-encoding",
   "metadata": {},
   "source": [
    "Дальнейшие эксперименты будет проводить на [наборе данных](https://www.kaggle.com/datasets/piyushagni5/white-wine-quality) о качестве белого вина.\n",
    "\n",
    "**Задание 2.2 (3 балла)**. Будем тестировать оптимизаторы на (барабанная дробь...) бустингах. Выберите библиотеку градиентного бустинга, которая приходится вам по душе и задайте распределения на гиперпараметры. Среди ваших гиперпараметров обязательно должны быть число деревьев, глубина дерева и learning rate, также добавьте еще 4-5 гиперпараметров по вашему желанию (среди них должен быть хотя бы один категориальный). Подумайте, какие распределения лучше всего подходят для каждого гиперпараметра.\n",
    "\n",
    "Как и всегда, разделите выборку на обучение и тест, предсказывать будем рейтинг вина (колонка `quality`). Хоть рейтинги и можно упорядочить, давайте решим задачу многоклассовой классификации на этих данных. В качестве метрики качества возьмем log loss. Мы будем сравнивать две стратегии для поиска, реализованные вами и [`OptunaSearchCV`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.OptunaSearchCV.html#optuna.integration.OptunaSearchCV) из библиотеки `optuna`, который имеет такой же интерфейс. Давайте ограничим наш бюджет поиска 40 итерациями. Обучите каждый из этих трех методов по нескольку раз, чтобы посмотреть на дисперсию метрики (как минимум 3 раза, а в идеале раз 5, если позволяет время)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (⊃｡•́‿•̀｡)⊃"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-float",
   "metadata": {},
   "source": [
    "Теперь попробуем визуализировать, что у нас получилось. Сделайте 3 сабплота, по одному на каждую стратегию поиска. На каждом графике изобразите лучшее найденное значение метрики, усредненное по запускам алгоритма, против номера итерации. Также нарисуйте область ± одного стандартного отклонения лучшего значения метрики. Кроме того, на том же графике сделайте scatter-plot **(номер итерации, значение метрики)** для этого запуска (именно для текущего набора гиперпараметра с итерации, а не лучшее, как для прошлого графика). Можете рисовать scatter-plot для всех повторных запусков сразу.\n",
    "\n",
    "Прокомментируйте наблюдаемое. Какой(-ие) алгоритм(-ы) показали себя лучше всего? Какой(-ие) можно назвать более стабильным(-и)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (⊃｡•́‿•̀｡)⊃"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-notion",
   "metadata": {},
   "source": [
    "**Задание 2.3 (1.5 балла).** Теперь посмотрим на распределение $p(y|x)$, которое получилось у `GPOptimizer`. Рассмотрим плоскость значений гиперпараметров числа деревьев и learning rate. Настройте ГП на выборку истории значений этих двух гиперпараметров одного из `GPOptimizer`, обученных вами выше. Ядро возьмите такое же, какое использовалось при поиске. Визуализируйте среднее, которое предсказывает ГП на плоскости (вам поможет `plt.imshow` или `plt.scatter`). Также добавьте на рисунок точки из истории поиска. Не забудьте про colorbar'ы и правильные подписи отметок на осях. Сделайте выводы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (⊃｡•́‿•̀｡)⊃"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-observation",
   "metadata": {},
   "source": [
    "**Задание 2.5 (1.5 балла)**. И на последок визуализируем, как изменялись распределения для категориального гиперпараметра. Для `GPOptimizer` постройте графики $\\mu(c) \\pm \\sigma(c)$ для каждой категории $c$ против номера итерации (все на одном рисунке). \n",
    "\n",
    "Прокомментируйте то, что получилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (⊃｡•́‿•̀｡)⊃"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
